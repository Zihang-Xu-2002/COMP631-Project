{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T09:19:55.137370Z",
     "start_time": "2025-02-25T09:19:55.022680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cloudscraper\n",
    "import time\n",
    "import random\n",
    "\n",
    "# 目标链接：第2页 IT 职位，地点：Houston, TX（URL 中参数已做编码）\n",
    "# URL = \"https://weworkremotely.com/\"\n",
    "URL = \"https://www.simplyhired.com/sitemap/viewjob/sitemap_index.xml\"\n",
    "\n",
    "# 请求头，模拟真实浏览器\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Referer\": \"https://www.monster.com/\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "}\n",
    "\n",
    "def get_page(url):\n",
    "    \"\"\"\n",
    "    使用 cloudscraper 发起 HTTP 请求，并返回整个页面的 HTML 数据。\n",
    "    这种方式不执行 JavaScript，仅获取服务器返回的静态 HTML。\n",
    "    \"\"\"\n",
    "    scraper = cloudscraper.create_scraper(browser={\n",
    "        \"browser\": \"chrome\",\n",
    "        \"platform\": \"windows\",\n",
    "        \"mobile\": False\n",
    "    })\n",
    "    try:\n",
    "        response = scraper.get(url, headers=HEADERS, timeout=15)\n",
    "        print(\"Status code:\", response.status_code)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            print(\"请求失败，状态码:\", response.status_code)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(\"请求异常:\", e)\n",
    "        return None\n",
    "\n",
    "def save_to_file(data, filename=\"monster_page.html\"):\n",
    "    \"\"\"\n",
    "    将数据写入文件，保存于当前目录。\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        print(\"没有数据可写入。\")\n",
    "        return\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(data)\n",
    "        print(\"成功将网页数据写入文件:\", filename)\n",
    "    except Exception as e:\n",
    "        print(\"写入文件失败:\", e)\n",
    "\n",
    "def main():\n",
    "    print(\"[信息] 开始爬取：\", URL)\n",
    "    html = get_page(URL)\n",
    "    if html:\n",
    "        # 可选：休眠一段随机时间，模拟正常访问\n",
    "        time.sleep(random.uniform(2, 5))\n",
    "        save_to_file(html, \"monster_page.html\")\n",
    "        # 打印部分网页内容以供检查（如前1000字符）\n",
    "        print(\"[信息] 网页前1000字符预览：\\n\", html[:1000])\n",
    "    else:\n",
    "        print(\"[信息] 未获取到网页数据。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "cbc313f0c3968aec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[信息] 开始爬取： https://www.simplyhired.com/sitemap/viewjob/sitemap_index.xml\n",
      "Status code: 403\n",
      "请求失败，状态码: 403\n",
      "[信息] 未获取到网页数据。\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:52:48.283167Z",
     "start_time": "2025-02-25T10:52:42.413164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cloudscraper\n",
    "import time\n",
    "import random\n",
    "\n",
    "# URL 模板，其中 {} 用于格式化页码\n",
    "URL_TEMPLATE = \"https://www.monster.com/jobs/q-it-jobs?page={}&so=p.h.p&where=Houston%2C+TX\"\n",
    "\n",
    "# 请求头，模拟真实浏览器\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Referer\": \"https://www.monster.com/\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "}\n",
    "\n",
    "def get_page(url):\n",
    "    \"\"\"\n",
    "    使用 cloudscraper 发起 HTTP 请求，并返回整个页面的 HTML 数据。\n",
    "    此方法仅获取服务器返回的静态 HTML（不执行 JavaScript）。\n",
    "    \"\"\"\n",
    "    scraper = cloudscraper.create_scraper(browser={\n",
    "        \"browser\": \"chrome\",\n",
    "        \"platform\": \"windows\",\n",
    "        \"mobile\": False\n",
    "    })\n",
    "    try:\n",
    "        response = scraper.get(url, headers=HEADERS, timeout=15)\n",
    "        print(\"Status code:\", response.status_code)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            print(\"请求失败，状态码:\", response.status_code)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(\"请求异常:\", e)\n",
    "        return None\n",
    "\n",
    "def save_to_file(data, filename):\n",
    "    \"\"\"\n",
    "    将数据写入文件，保存于当前目录。\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        print(\"没有数据可写入。\")\n",
    "        return\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(data)\n",
    "        print(\"成功将网页数据写入文件:\", filename)\n",
    "    except Exception as e:\n",
    "        print(\"写入文件失败:\", e)\n",
    "\n",
    "def main():\n",
    "    for page in range(1, 21):\n",
    "        # 构造当前页的 URL\n",
    "        url = URL_TEMPLATE.format(page)\n",
    "        print(\"[信息] 开始爬取第 {} 页: {}\".format(page, url))\n",
    "        html = get_page(url)\n",
    "        if html:\n",
    "            filename = f\"monster_page_{page}.html\"\n",
    "            save_to_file(html, filename)\n",
    "            # 打印当前页面前1000字符预览\n",
    "            print(\"[信息] 第 {} 页网页前1000字符预览：\\n{}\".format(page, html[:1000]))\n",
    "        else:\n",
    "            print(\"[信息] 未获取到第 {} 页的网页数据。\".format(page))\n",
    "        # 随机休眠 2-5 秒，模拟正常访问\n",
    "        time.sleep(random.uniform(2, 5))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "5e8a095ee9db3d1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[信息] 开始爬取第 1 页: https://www.monster.com/jobs/q-it-jobs?page=1&so=p.h.p&where=Houston%2C+TX\n",
      "Status code: 200\n",
      "成功将网页数据写入文件: monster_page_1.html\n",
      "[信息] 第 1 页网页前1000字符预览：\n",
      "<!DOCTYPE html><html lang=\"en-US\"><head><meta charSet=\"utf-8\"/><meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\"/><meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"/><meta name=\"referrer\" content=\"always\"/><meta name=\"affiliate\" content=\"jobs\"/><meta name=\"google-site-verification\" content=\"hdRwRevikG3Zl1Ju07n-CT4cHgNgEBwY1J0h6mFcmv4\"/><meta name=\"x-forwarded-host\" content=\"monster.com\"/><meta name=\"origin-host\" content=\"monster.com\"/><link data-testid=\"relCanonical\" rel=\"canonical\" href=\"https://www.monster.com/jobs/q-it-jobs\"/><link rel=\"next\" href=\"https://www.monster.com/jobs/q-it-jobs?page=2\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, maximum-scale=5.0, minimum-scale=1.0\"/><title lang=\"en-US\" aria-label=\"All the IT Jobs Hiring Now &amp;#128176; | Monster\">All the IT Jobs Hiring Now &amp;#128176; | Monster</title><meta name=\"Description\" content=\"IT jobs near you need filling now! Search and apply to your dream roles today wit\n",
      "[信息] 开始爬取第 2 页: https://www.monster.com/jobs/q-it-jobs?page=2&so=p.h.p&where=Houston%2C+TX\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 70\u001B[0m\n\u001B[1;32m     67\u001B[0m         time\u001B[38;5;241m.\u001B[39msleep(random\u001B[38;5;241m.\u001B[39muniform(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m5\u001B[39m))\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 70\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[19], line 58\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     56\u001B[0m url \u001B[38;5;241m=\u001B[39m URL_TEMPLATE\u001B[38;5;241m.\u001B[39mformat(page)\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[信息] 开始爬取第 \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m 页: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(page, url))\n\u001B[0;32m---> 58\u001B[0m html \u001B[38;5;241m=\u001B[39m \u001B[43mget_page\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m html:\n\u001B[1;32m     60\u001B[0m     filename \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmonster_page_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpage\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "Cell \u001B[0;32mIn[19], line 28\u001B[0m, in \u001B[0;36mget_page\u001B[0;34m(url)\u001B[0m\n\u001B[1;32m     22\u001B[0m scraper \u001B[38;5;241m=\u001B[39m cloudscraper\u001B[38;5;241m.\u001B[39mcreate_scraper(browser\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbrowser\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchrome\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mplatform\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwindows\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmobile\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     26\u001B[0m })\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 28\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mscraper\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mHEADERS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m15\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStatus code:\u001B[39m\u001B[38;5;124m\"\u001B[39m, response\u001B[38;5;241m.\u001B[39mstatus_code)\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m200\u001B[39m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/COMP631/lib/python3.10/site-packages/requests/sessions.py:602\u001B[0m, in \u001B[0;36mSession.get\u001B[0;34m(self, url, **kwargs)\u001B[0m\n\u001B[1;32m    594\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001B[39;00m\n\u001B[1;32m    595\u001B[0m \n\u001B[1;32m    596\u001B[0m \u001B[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[1;32m    597\u001B[0m \u001B[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001B[39;00m\n\u001B[1;32m    598\u001B[0m \u001B[38;5;124;03m:rtype: requests.Response\u001B[39;00m\n\u001B[1;32m    599\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    601\u001B[0m kwargs\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m--> 602\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mGET\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/COMP631/lib/python3.10/site-packages/cloudscraper/__init__.py:259\u001B[0m, in \u001B[0;36mCloudScraper.request\u001B[0;34m(self, method, url, *args, **kwargs)\u001B[0m\n\u001B[1;32m    246\u001B[0m     (method, url, args, kwargs) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequestPreHook(\n\u001B[1;32m    247\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    248\u001B[0m         method,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    251\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    252\u001B[0m     )\n\u001B[1;32m    254\u001B[0m \u001B[38;5;66;03m# ------------------------------------------------------------------------------- #\u001B[39;00m\n\u001B[1;32m    255\u001B[0m \u001B[38;5;66;03m# Make the request via requests.\u001B[39;00m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;66;03m# ------------------------------------------------------------------------------- #\u001B[39;00m\n\u001B[1;32m    258\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecodeBrotli(\n\u001B[0;32m--> 259\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mperform_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    260\u001B[0m )\n\u001B[1;32m    262\u001B[0m \u001B[38;5;66;03m# ------------------------------------------------------------------------------- #\u001B[39;00m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# Debug the request via the Response object.\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# ------------------------------------------------------------------------------- #\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdebug:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/COMP631/lib/python3.10/site-packages/cloudscraper/__init__.py:192\u001B[0m, in \u001B[0;36mCloudScraper.perform_request\u001B[0;34m(self, method, url, *args, **kwargs)\u001B[0m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mperform_request\u001B[39m(\u001B[38;5;28mself\u001B[39m, method, url, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 192\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mCloudScraper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/COMP631/lib/python3.10/site-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    587\u001B[0m }\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m/opt/anaconda3/envs/COMP631/lib/python3.10/site-packages/requests/sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[1;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43madapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[1;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[0;32m/opt/anaconda3/envs/COMP631/lib/python3.10/site-packages/requests/adapters.py:667\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    664\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m TimeoutSauce(connect\u001B[38;5;241m=\u001B[39mtimeout, read\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[1;32m    666\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 667\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    668\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    669\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    670\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    671\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    672\u001B[0m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    673\u001B[0m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    674\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    675\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    676\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    677\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    678\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    679\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    681\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    682\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/COMP631/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    784\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    786\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[0;32m--> 787\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    788\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    789\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    800\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    802\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n\u001B[1;32m    803\u001B[0m clean_exit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/COMP631/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    532\u001B[0m \u001B[38;5;66;03m# Receive the response from the server\u001B[39;00m\n\u001B[1;32m    533\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 534\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    535\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (BaseSSLError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    536\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mread_timeout)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/COMP631/lib/python3.10/site-packages/urllib3/connection.py:516\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    513\u001B[0m _shutdown \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshutdown\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    515\u001B[0m \u001B[38;5;66;03m# Get the response from http.client.HTTPConnection\u001B[39;00m\n\u001B[0;32m--> 516\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    518\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    519\u001B[0m     assert_header_parsing(httplib_response\u001B[38;5;241m.\u001B[39mmsg)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/COMP631/lib/python3.10/http/client.py:1375\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1373\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1374\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1375\u001B[0m         \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbegin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1376\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n\u001B[1;32m   1377\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/COMP631/lib/python3.10/http/client.py:318\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;66;03m# read until we get a non-100 response\u001B[39;00m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 318\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n\u001B[1;32m    320\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/COMP631/lib/python3.10/http/client.py:279\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 279\u001B[0m     line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_MAXLINE\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miso-8859-1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) \u001B[38;5;241m>\u001B[39m _MAXLINE:\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m LineTooLong(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus line\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/COMP631/lib/python3.10/socket.py:717\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    715\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    716\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 717\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    718\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    719\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/COMP631/lib/python3.10/ssl.py:1307\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[0;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[1;32m   1303\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1304\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1305\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m   1306\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[0;32m-> 1307\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1308\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1309\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/COMP631/lib/python3.10/ssl.py:1163\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1161\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1162\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1163\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1164\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1165\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T08:32:09.126389Z",
     "start_time": "2025-02-25T08:32:01.470466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cloudscraper\n",
    "import time\n",
    "import random\n",
    "\n",
    "# URL 模板，其中 {} 用于格式化页码\n",
    "URL_TEMPLATE = \"https://www.monster.com/jobs/q-it-jobs?page={}&so=p.h.p&where=Houston%2C+TX\"\n",
    "\n",
    "# 请求头，模拟真实浏览器\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"sec-ch-ua\": \"\\\"Not.A/Brand\\\";v=\\\"8\\\", \\\"Chromium\\\";v=\\\"115\\\", \\\"Google Chrome\\\";v=\\\"115\\\"\",\n",
    "    \"sec-ch-ua-mobile\": \"?0\",\n",
    "    \"sec-ch-ua-platform\": \"\\\"Windows\\\"\"\n",
    "}\n",
    "\n",
    "def get_page(url):\n",
    "    \"\"\"\n",
    "    使用 cloudscraper 发起 HTTP 请求，并返回整个页面的 HTML 数据。\n",
    "    此方法仅获取服务器返回的静态 HTML（不执行 JavaScript）。\n",
    "    \"\"\"\n",
    "    scraper = cloudscraper.create_scraper(browser={\n",
    "        \"browser\": \"chrome\",\n",
    "        \"platform\": \"windows\",\n",
    "        \"mobile\": False\n",
    "    })\n",
    "    try:\n",
    "        response = scraper.get(url, headers=HEADERS, timeout=15)\n",
    "        print(\"Status code:\", response.status_code)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            print(\"请求失败，状态码:\", response.status_code)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(\"请求异常:\", e)\n",
    "        return None\n",
    "\n",
    "def save_to_file(data, filename):\n",
    "    \"\"\"\n",
    "    将数据写入文件，保存于当前目录。\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        print(\"没有数据可写入。\")\n",
    "        return\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(data)\n",
    "        print(\"成功将网页数据写入文件:\", filename)\n",
    "    except Exception as e:\n",
    "        print(\"写入文件失败:\", e)\n",
    "\n",
    "def main():\n",
    "    for page in range(1, 21):\n",
    "        # 构造当前页的 URL\n",
    "        url = URL_TEMPLATE.format(page)\n",
    "        print(\"[信息] 开始爬取第 {} 页: {}\".format(page, url))\n",
    "        html = get_page(url)\n",
    "        if html:\n",
    "            filename = f\"monster_page_{page}_.html\"\n",
    "            save_to_file(html, filename)\n",
    "            # 打印当前页面前1000字符预览\n",
    "            print(\"[信息] 第 {} 页网页前1000字符预览：\\n{}\".format(page, html[:1000]))\n",
    "        else:\n",
    "            print(\"[信息] 未获取到第 {} 页的网页数据。\".format(page))\n",
    "        # 随机休眠 2-5 秒，模拟正常访问\n",
    "        time.sleep(random.uniform(2, 5))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "1f1216b5e92d8434",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[信息] 开始爬取第 1 页: https://www.monster.com/jobs/q-it-jobs?page=1&so=p.h.p&where=Houston%2C+TX\n",
      "Status code: 403\n",
      "请求失败，状态码: 403\n",
      "[信息] 未获取到第 1 页的网页数据。\n",
      "[信息] 开始爬取第 2 页: https://www.monster.com/jobs/q-it-jobs?page=2&so=p.h.p&where=Houston%2C+TX\n",
      "Status code: 403\n",
      "请求失败，状态码: 403\n",
      "[信息] 未获取到第 2 页的网页数据。\n",
      "[信息] 开始爬取第 3 页: https://www.monster.com/jobs/q-it-jobs?page=3&so=p.h.p&where=Houston%2C+TX\n",
      "Status code: 403\n",
      "请求失败，状态码: 403\n",
      "[信息] 未获取到第 3 页的网页数据。\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 75\u001B[0m\n\u001B[1;32m     72\u001B[0m         time\u001B[38;5;241m.\u001B[39msleep(random\u001B[38;5;241m.\u001B[39muniform(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m5\u001B[39m))\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 75\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[8], line 72\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[信息] 未获取到第 \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m 页的网页数据。\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(page))\n\u001B[1;32m     71\u001B[0m \u001B[38;5;66;03m# 随机休眠 2-5 秒，模拟正常访问\u001B[39;00m\n\u001B[0;32m---> 72\u001B[0m \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muniform\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T08:08:33.378875Z",
     "start_time": "2025-02-25T08:08:33.366763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import asyncio\n",
    "import random\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# URL 模板，其中 {} 用于格式化页码\n",
    "URL_TEMPLATE = \"https://www.monster.com/jobs/q-it-jobs?page={}&so=p.h.p&where=Houston%2C+TX\"\n",
    "\n",
    "# 请求头，模拟真实浏览器（在 Playwright 中部分头信息需要单独设置）\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Referer\": \"https://www.monster.com/\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "}\n",
    "\n",
    "async def crawl_page(page_num: int, context) -> None:\n",
    "    \"\"\"\n",
    "    打开指定页码的 URL，等待页面加载完成后获取 HTML 内容，并保存到文件。\n",
    "    \"\"\"\n",
    "    url = URL_TEMPLATE.format(page_num)\n",
    "    print(f\"[INFO] 开始爬取第 {page_num} 页: {url}\")\n",
    "    page = await context.new_page()\n",
    "    \n",
    "    # 随机等待 2-5 秒，模拟正常浏览器访问\n",
    "    await asyncio.sleep(random.uniform(2, 5))\n",
    "    \n",
    "    # 导航到目标页面，并等待网络空闲状态，确保 JS 动态加载完成\n",
    "    await page.goto(url, wait_until=\"networkidle\")\n",
    "    # 可选：额外等待几秒钟确保所有异步请求完成\n",
    "    await asyncio.sleep(2)\n",
    "    \n",
    "    content = await page.content()\n",
    "    filename = f\"monster_page_{page_num}.html\"\n",
    "    Path(filename).write_text(content, encoding=\"utf-8\")\n",
    "    print(f\"[INFO] 第 {page_num} 页已保存到文件：{filename}\")\n",
    "    \n",
    "    await page.close()\n",
    "\n",
    "async def main():\n",
    "    async with async_playwright() as p:\n",
    "        # 启动无头浏览器\n",
    "        browser = await p.firefox.launch(headless=True)\n",
    "        # 创建一个新的浏览器上下文，并设置自定义请求头和 User-Agent\n",
    "        context = await browser.new_context(\n",
    "            user_agent=HEADERS[\"User-Agent\"],\n",
    "            extra_http_headers={\n",
    "                \"Accept\": HEADERS[\"Accept\"],\n",
    "                \"Accept-Language\": HEADERS[\"Accept-Language\"],\n",
    "                \"Referer\": HEADERS[\"Referer\"],\n",
    "                \"Connection\": HEADERS[\"Connection\"],\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        tasks = []\n",
    "        # 限制并发数，避免请求过快引起 403，可以适当调整并发数\n",
    "        semaphore = asyncio.Semaphore(5)\n",
    "\n",
    "        async def sem_crawl(page_num: int):\n",
    "            async with semaphore:\n",
    "                await crawl_page(page_num, context)\n",
    "\n",
    "        # 假设我们爬取 1 到 20 页\n",
    "        for page_num in range(1, 21):\n",
    "            tasks.append(asyncio.create_task(sem_crawl(page_num)))\n",
    "        \n",
    "        # 并发执行所有任务\n",
    "        await asyncio.gather(*tasks)\n",
    "        \n",
    "        await context.close()\n",
    "        await browser.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    "
   ],
   "id": "b646313d760c734c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z6/8hw5tw4s1r90140zc3s6kg580000gn/T/ipykernel_22446/3808264955.py:78: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  main()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "90b2f4cd54913521"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6a3ad89e201e1870"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
